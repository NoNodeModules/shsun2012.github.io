---
layout: post
title: "Basic statistics with R"
subtitle: "Correlation, PCA, PCR"
author: "Shsun"
header-style: text
tags:
  - R
  - PCA
---
Currently I am analysing meteorological impacts on stomatal conductance as well as how these interactions would affect surface ozone contrations. Previously I listed many variables and tried to find the patterns, using MERRA2 meteorology and surface ozone concentrations to analyse meteorology impacts on ozone in US. Given MERRA2 meteorology, SPEI data, surface, ozone concentrations, use following steps to practice basic data analysis in R:
1. Calculate the moving average of meteorological variables and get the deseasonalized daily values.
2. Do regression between ozone and meteorology.
3. Calculate PCA for each grid
4. Plot maps of PC with ggplot

##### Linear regression
Pre-work before regression or correlation is important. The first two steps include calculating moving averages (MA) and correlations. Moving averages can smooth out the noise of random outliers and emphasize long-term trends. In this case, we subtract MA from daily values to remove the long-term trend and focus on deseasonalized day-to-day variablities. Normalization of variables is also necessary, because the original variables may have different scales. Performing PCA on un-normalized variables will lead to insanely large loadings for variables with high variance. In turn, this would lead to dependence of a PC on the variable with high variance. This can easily be done in R.

Then we use the deseasonalized daily values to calculate correlations. Now we can use `lm` or `glm` for regression analysis.  `lm` fits models with the form `Y = Xb + e`, while `glm` fits models with the form `f(Y) = Xb + e`. The `glm` generalizes the linear model into the expotential family, while `lm` assumes data following a specific distribution: Normal or Gauss. `lm.beta` adds the standarlized regression coefficients to objects created by `lm`.
```
reg = lm(O3 ~ T2M + QV2M + PRECTOT + GWETROOT + SLP + WIND + SWGDN + CLDTOT, na.action=na.exclude)
reg.st = lm.beta(reg)
```

##### Model selection
###### Forward selection
First we start with the linear regression model `lm` with the most significant independent variable. Then we extend the model with the remaining independent variables one at a time. We stop when there is no significant extensions anymore.

```
summary(lm(O3 ~ T2M + WIND))
summary(lm(O3 ~ T2M + WIND + QV2M))  # add variable and check p-value
```
###### Backward selection
Start with the full modeland remove the most insignificant independent variable. Finally we stop when all p values are significant.

#####  PCA analysis
The fundamentals of PCA have been discussed over and over again. The aim of PCA is to create a new set of uncorrelated variables that are a linear combination of the initial variables and explain as much of the initial variation as possible. When analysing meteorological variables, we always use PCA to extract the uncorrelated principal components as original meteorolgical factors are usually correlated with each other. Basically, PC1 is a linear combination of orignial predictor variables which captures the maximum variance in the dataset. It determines the direction of highest variablity in the data. PC2 is also a linear combination of original predictors which captures the remaining variance in the dataset and is uncorrelated with PC1.
```
pca = prcomp(~X.data, scale=TRUE, na.action=na.exclude)
PC.sd = pca$sdev
PC.rot = pca$rotation
PC.data = pca$x
pca.var.per = round(pca.var/sum(pca.var)*100, 1)

```
By default, `prcomp()` expects samples (observations) to be rows and the variables to be columns. returns `x`, `sdev`, and `rotation`. **x** contains the PCs for grawing a graph. `plot(pca$x[,1], pca$x[,2])` can be used to plot for the first two PCs. The x and y axis tells about he percentage of the variatioin in the original data that PC1 and PC2 account for. `pca$rotation` record loading scores for each PC. Loading scores can be used to determine which variables have the largest effects. `pca$rotation[,1]` is the loading scores for PC1. For example, a PCA ggplot for stomatal conductnace vesus some environmental variables:

![PCA](/img/pca.png)

It shows that vapour pressure deficit of air (VPDair) and temperature of air (T air) explain most of the variance in the PC1, while ustar explaines most of the variance in PC2. One of the disadventage of PC is interpreting the PCs. When the PC1 is highly correlated with all variables, we can say it summarizes the data well and that instead of using the meteorological variables to make prediction, we can just use PC1. If two meteorological variables are correlated with each other, perhaps usign one new variable (i.e. PC1) can summarize both variables.

##### PCR analysis
We also want to know whether the new PCs can predict the phenomena well after PCA analysis. We use `lm()` to see if PC1 ~ PC4 can predict Y.n (i.e., ozone) well.   
```
PC.reg = lm(Y.n ~ -1 + PC1 + PC2 + PC3 + PC4, na.action = na.exclude)

```
To see how well this model can predict ozone, we can use `fitted()` function to see the prediction.
```
fitted(PC.reg)
```
##### Factor analysis
The intuition of PCA is to find new combination of variables which form larger variances, while FA is to find hidden variables which affect your observed variables by looking at the correlation. FA in R is easy to dy by a function `factanal()`. The following assumes the number of hidden variables is 1.
```
fa = factanal(data, factor=1)
```
data is a dataframe of variables, each variable is a column. The model gets improved if you have more variables, which shows the trade-off between the number of variables and the accuracy of the model. Similar to PCA, we can look at the cumulative portion of variance, and if that reaches some numbers, we can stop adding more factors. The Kaiser rule is to discard components whose eigenvalues are below 1.0.
```
ev = eigen(cor(data))
ev$values
```
By looking at how many values are over 1.0, we can decide the number of factors. There is no definitive way to determine the number of factors.

Another method is to use variance inflation factor (VIF,[wiki](https://en.wikipedia.org/wiki/Variance_inflation_factor)). VIF detects multicollinearity in regression analysis. A VIF value less than 1 is defined as non-correlated, while a VIF greater than 5 means highly correlated.


##### Reference
1. [Principal Components Regression](https://rstudio-pubs-static.s3.amazonaws.com/222571_7b65a75ec1214b56bccafa79e8c7f9ed.html)
2. [Factor Analysis](http://yatani.jp/teaching/doku.php?id=hcistats:fa)
3. [Data mining - Principal Component (Analysis|Regression) (PCA)](https://gerardnico.com/data_mining/pca)
4. [Practical Guide to Principal Component Analysis (PCA) in R & Python](https://www.analyticsvidhya.com/blog/2016/03/practical-guide-principal-component-analysis-python/)
5. [Variance Inflation Factor](https://www.statisticshowto.datasciencecentral.com/variance-inflation-factor/)
